{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c8af91f-7457-45f9-a558-b29f2bffe90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import mmread\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8879de7-338d-4174-a61f-6b0cdc8dcfcf",
   "metadata": {},
   "source": [
    "# Create new training, test and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f1568-0c38-491f-8817-b097fa95e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datasplit files\n",
    "file_train = pd.read_csv('/home/icb/alessandro.palma/imCPA/data/metadata/datasplit1-train.csv')\n",
    "file_test = pd.read_csv('/home/icb/alessandro.palma/imCPA/data/metadata/datasplit1-test.csv')\n",
    "file_val = pd.read_csv('/home/icb/alessandro.palma/imCPA/data/metadata/datasplit1-val.csv')\n",
    "\n",
    "unseen_examples = pd.read_csv('/home/icb/alessandro.palma/imCPA/data/metadata/unseen_examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21eeb7c-a194-4429-8ff6-2cc4c248168c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767e5269-72ef-4309-8471-50202a92a725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['boldine', 'naproxen'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(file_val.CPD_NAME, file_test.CPD_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "823e8ae8-57b3-44ee-84db-4b8aec3d3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the whole data\n",
    "dataset = pd.concat([file_train, file_test, file_val], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30964f2d-ead6-48e3-a2a5-50f8c1973d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SAMPLE_KEY', 'BROAD_ID', 'PLATE_ID', 'WELL_POSITION', 'SITE',\n",
      "       'SAMPLE_ID', 'CPD_NAME', 'CPD_NAME_TYPE', 'SMILES', 'INCHIKEY',\n",
      "       'IMG_ERSyto', 'IMG_ERSytoBleed', 'IMG_Hoechst', 'IMG_Mito',\n",
      "       'IMG_Ph_golgi', 'IMG_CNT_CELLS', 'ROW_NR_LABEL_MAT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check column names of the new dataset \n",
    "print(file_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "281d650d-34a9-49e8-a50c-093199764005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10560"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 8.5K molecules that will be seen from the training set\n",
    "molecules = np.unique(dataset.CPD_NAME)\n",
    "len(np.unique(dataset.CPD_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d36175ee-c74a-4469-854b-bf2574d3e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the seen molecules \n",
    "n = len(dataset)\n",
    "train_perc, test_perc, valid_perc = np.round(n*0.80), np.round(n*0.20),  np.round(n*0.10)\n",
    "\n",
    "# Each molecule counts either 48 or 24 entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23e42c85-fd3e-48f7-b884-b97768565b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "unseen = list(np.unique(unseen_examples.CPD_NAME))\n",
    "seen = list(set(dataset.CPD_NAME)-set(unseen)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d6b51f-a3e7-4241-8a29-0c89d9ad77a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the seen and unseen do not collide\n",
    "set(seen).intersection(unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85daed8b-315d-4662-973f-b9ae7163d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_rows = [i for i in range(n) if dataset.CPD_NAME[i] in seen]\n",
    "unseen_rows = [i for i in range(n) if dataset.CPD_NAME[i] not in seen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f368f90a-5f74-414e-9561-60d01a071e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/home/icb/alessandro.palma/imCPA/data/metadata_processed/seen_unseen_compounds.npz', seen = seen, unseen = unseen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44fbd6b1-cc5d-4448-8c4e-dbeddaa41e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and validation set \n",
    "np.random.seed(42)\n",
    "def split_train_val_test(df, seen, unseen):\n",
    "    # Indices for training, test and validation indexes \n",
    "    train_set, test_set, valid_set, ood_set = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    # For each molecule shared\n",
    "    for seen_mol in seen:\n",
    "        # Slice of dataset for a molecule\n",
    "        ds_slice = dataset.iloc[dataset.CPD_NAME.values == seen_mol]\n",
    "        n_slice = len(ds_slice)\n",
    "        n_train, n_test, n_valid = int(np.round(n_slice*0.70)), int(np.round(n_slice*0.20)), int(np.round(n_slice*0.10))\n",
    "        # Separate the slice observations belonging to the three sets (randomly)\n",
    "        df_slice_shuffled = ds_slice.sample(frac=1)\n",
    "        slice_train = df_slice_shuffled.iloc[0:n_train+1]\n",
    "        slice_test = df_slice_shuffled.iloc[(n_train+1):(n_train+n_test+1)]\n",
    "        slice_valid = df_slice_shuffled.iloc[(n_train+n_test+1):]\n",
    "        # Stack the datasets under the empty frames\n",
    "        train_set  = pd.concat([train_set, slice_train])\n",
    "        test_set = pd.concat([test_set, slice_test])\n",
    "        valid_set = pd.concat([valid_set, slice_valid])\n",
    "    \n",
    "    for unseen_mol in unseen:\n",
    "        ds_slice = dataset.iloc[dataset.CPD_NAME.values == unseen_mol]\n",
    "        ood_set = pd.concat([ood_set, ds_slice])\n",
    "    # Set the column names\n",
    "    train_set.columns, test_set.columns, valid_set.columns, ood_set.columns = df.columns, df.columns, df.columns, df.columns\n",
    "    return train_set, test_set, valid_set, ood_set\n",
    "\n",
    "train_set, test_set, valid_set, ood_set = split_train_val_test(dataset, seen, unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6885109-888e-4d7e-98f1-b1bef4e03351",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv('/home/icb/alessandro.palma/imCPA/data/metadata_processed/datasplit-train.csv')\n",
    "test_set.to_csv('/home/icb/alessandro.palma/imCPA/data/metadata_processed/datasplit-test.csv')\n",
    "valid_set.to_csv('/home/icb/alessandro.palma/imCPA/data/metadata_processed/datasplit-val.csv')\n",
    "ood_set.to_csv('/home/icb/alessandro.palma/imCPA/data/metadata_processed/datasplit-ood.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40029292-f20e-4835-b0ad-d0f0729bc421",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create training, test and valdation npy files with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "511ec1ca-8a80-4389-b82a-16d4d1e46b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files again if not in memory\n",
    "# Get datasplit files\n",
    "file_train = pd.read_csv('/home/icb/alessandro.palma/imCPA/data/metadata_processed/datasplit-train.csv')\n",
    "file_test = pd.read_csv('/home/icb/alessandro.palma/imCPA/data/metadata_processed/datasplit-test.csv')\n",
    "file_val = pd.read_csv('/home/icb/alessandro.palma/imCPA/data/metadata_processed/datasplit-val.csv')\n",
    "file_ood = pd.read_csv('/home/icb/alessandro.palma/imCPA/data/metadata_processed/datasplit-ood.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9aa0d08-b2b4-4f95-a9f9-6df51cb0de2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(file_train.SAMPLE_KEY.values, file_val.SAMPLE_KEY.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c829cf-53fb-440e-98ed-75124175053d",
   "metadata": {},
   "source": [
    "### Assay labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96dd8be0-7f1e-4cc7-afd0-0a035a1ccfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the label matrix\n",
    "label_mat = mmread('/home/icb/alessandro.palma/imCPA/data/metadata/label_matrix/label-matrix.mtx').tocsr()\n",
    "col_labels = pd.read_csv('/home/icb/alessandro.palma/imCPA/data/metadata/label_matrix/column-assay-index.csv')\n",
    "row_labels = pd.read_csv('/home/icb/alessandro.palma/imCPA/data/metadata/label_matrix/row-compound-index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85e9de0b-8557-4d7b-a0a4-d6207338521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_assay_label_to_samples(file,\n",
    "                        label_mat):\n",
    "    \"\"\"\n",
    "    To each observation add its vector of labels \n",
    "    \"\"\"\n",
    "    file_rows = file.ROW_NR_LABEL_MAT  # Row on the assay label matrix \n",
    "    y_assays = []  # List of the assay labels for the dataset \n",
    "    for row in file_rows:\n",
    "        y_assay = label_mat[row].todense()\n",
    "        y_assays.append(np.array(y_assay))\n",
    "    return np.array(y_assays).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96b6849f-1d9e-401c-8bdf-58f8188626c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_assay_labels = add_assay_label_to_samples(file_train, label_mat)\n",
    "test_assay_labels = add_assay_label_to_samples(file_test, label_mat)\n",
    "val_assay_labels = add_assay_label_to_samples(file_val, label_mat)\n",
    "ood_assay_labels = add_assay_label_to_samples(file_ood, label_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3055639-d400-45f5-a281-85f287e51f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204852"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_assay_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e961e8e-2432-4367-8191-b72068d6fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save labels\n",
    "np.savez('/home/icb/alessandro.palma/imCPA/data/metadata_processed/labels_train.npz', assay_labs = train_assay_labels)\n",
    "np.savez('/home/icb/alessandro.palma/imCPA/data/metadata_processed/labels_test.npz', assay_labs = test_assay_labels)\n",
    "np.savez('/home/icb/alessandro.palma/imCPA/data/metadata_processed/labels_valid.npz', assay_labs = val_assay_labels)\n",
    "np.savez('/home/icb/alessandro.palma/imCPA/data/metadata_processed/labels_ood.npz', assay_labs = ood_assay_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9a299-8aaa-4109-83b1-ea74107fa55f",
   "metadata": {},
   "source": [
    "## Prova "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c622deb7-ef64-4da9-bd1a-761055c85516",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/icb/alessandro.palma/data/splits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36ec20-f6a1-45fb-912a-ea67108ca3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellPaintingDataset:\n",
    "    \"\"\"\n",
    "    Dataset class for image data \n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, transform, device='cuda', return_labels=False):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        -------------\n",
    "            :data_path: the repository where the data is stored \n",
    "            :transform: a pytorch transform object to apply augmentation to the data \n",
    "            :data_index_path: path to .npz object with sample names, molecule names, molecule smiles and the assay labels\n",
    "            :return_labels: bool to assess whether to return labels together with observations in __getitem__\n",
    "        \"\"\"    \n",
    "        assert os.path.exists(data_path), 'The data path does not exist'\n",
    "        assert os.path.exists(drug_path), 'The drug path does not exist'\n",
    "\n",
    "        # Read train, validation and test sets \n",
    "        self.data_path = data_path \n",
    "        self.transform = transform \n",
    "        self.device = device \n",
    "        self.return_labels = return_labels\n",
    "\n",
    "        # Read the drug names\n",
    "        print('Load the data')\n",
    "        self.fold_datasets = self.read_folds()\n",
    "        \n",
    "        # Take the seen molecules from the training, test and valid set and map them to indices \n",
    "        seen_compounds = np.unique(self.fold_datasets['train']['mol_names'])\n",
    "        unseen_compounds = np.unique(self.fold_datasets['ood']['mol_names'])\n",
    "        assert len(seen_compounds)+len(unseen_compounds) == 10600\n",
    "        \n",
    "        seen_compounds = sorted(seen_compounds)\n",
    "        mol2label = {d:i for d,i in zip(seen_compunds, range(len(seen_compunds)))}\n",
    "        \n",
    "        # Onehot encoder \n",
    "        encoder_drug = OneHotEncoder(sparse=False, categories=[seen_compounds])\n",
    "        encoder_drug.fit(np.array(seen_compounds).reshape((-1,1)))\n",
    "        \n",
    "        # Initialize the datasets \n",
    "        fold_datasets = {'train': CellPaintingFold('train', self.fold_datasets['train'], encoder_drug, mol2label, self.transform, self.return_labels),\n",
    "                         'val': CellPaintingFold('val', self.fold_datasets['val'], encoder_drug, mol2label, self.transform, self.return_labels),\n",
    "                         'test': CellPaintingFold('test', self.fold_datasets['test'], encoder_drug, mol2label, self.transform, self.return_labels),\n",
    "                         'ood': CellPaintingFold('ood', self.fold_datasets['ood'], encoder_drug, mol2label, self.transform, self.return_labels)}\n",
    "        \n",
    "        \n",
    "    def read_folds(self):\n",
    "        \"\"\"\n",
    "        Extract the filenames of images in the train, test and validation sets from the \n",
    "        associated folder\n",
    "        \"\"\"\n",
    "        # Get the file names and molecules of training, test and validation sets\n",
    "        datasets = dict()\n",
    "        for fold_name in ['train', 'val', 'test', 'ood']:\n",
    "            datasets[fold_name] = {}\n",
    "            # Fetch the data\n",
    "            data_index_path = os.path.join(self.data_path, f'{fold_name}_data_index.npz')\n",
    "            # Get the files with the sample splits and add them to the dictionary \n",
    "            fold_file, mol_names, mol_smiles, assay_labels  = self.get_files_and_mols_from_path(data_index_path=data_index_path)\n",
    "\n",
    "            # Add the  important entries to the dataset\n",
    "            datasets[fold_name]['file_names'] = fold_file\n",
    "            datasets[fold_name]['mol_names'] = mol_names\n",
    "            datasets[fold_name]['mol_smiles'] = mol_smiles\n",
    "            datasets[fold_name]['assay_labels'] = assay_labels\n",
    "        return datasets\n",
    "    \n",
    "    def get_files_and_mols_from_path(self, data_index_path): \n",
    "        \"\"\"\n",
    "        Load object with image names, molecule names and smiles \n",
    "        -------------------\n",
    "        data_index_path: The path to the data index with information about molecules and sample names\n",
    "        \"\"\"\n",
    "        assert os.path.exists(data_index_path), 'The data index file does not exist'\n",
    "        # Load the index file \n",
    "        file = np.load(data_index_path, allow_pickle= True)\n",
    "\n",
    "        file_names = file['filenames']\n",
    "        mol_names = file['mol_names']\n",
    "        mol_smiles =  file['mol_smiles']\n",
    "        assay_labels = file['assay_labels']\n",
    "        return file_names, mol_names, mol_smiles, assay_labels\n",
    "    \n",
    "\n",
    "class CellPaintingFold(Dataset):\n",
    "    def __init__(fold, data, drug_encoder, mol2label, transform, return_labels = True):\n",
    "        super(CellPaintingDataset, self).__init__() \n",
    "        \n",
    "        # For each piece of the data create its own object\n",
    "        self.file_names = data['file_names']\n",
    "        self.mol_names = data['mol_names']\n",
    "        self.mol_smiles = data['mol_smiles']\n",
    "        self.assay_labels = data['assay_labels']\n",
    "        \n",
    "        self.drug_encoder = drug_encoder\n",
    "        self.mol2label = mol2label\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.return_labels = return_labels \n",
    "    \n",
    "        # One -hot encode molecules\n",
    "        self.one_hot_drugs = self.drug_encoder.transform(np.array(self.mol_names.reshape((-1,1))))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples \n",
    "        \"\"\"\n",
    "        return len(self.file_names)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate one example datapoint \n",
    "        \"\"\"\n",
    "        img_file = self.file_names[idx]\n",
    "        sample = img_file.split('-')[0]\n",
    "        well = img_file.split('-')[1].split('-')[0]\n",
    "        # Load image \n",
    "        with np.load(os.path.join(self.data_path, sample, well, f'{img_file}.npz'), allow_pickle = True) as f:\n",
    "            img = f['arr_0']\n",
    "        img = torch.from_numpy(img).to(torch.float)\n",
    "        img = img.permute(2,0,1)  # Place channel dimension in front of the others \n",
    "        if self.transform != None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        if self.return_labels:\n",
    "            return dict(X=img, \n",
    "                        file_name=img_file,\n",
    "                        mol_name=self.mol_names[idx], \n",
    "                        mol_one_hot=self.one_hot_drugs[idx]\n",
    "                        mol_smile=self.mol_smiles[idx],\n",
    "                        assay_labels=self.assay_labels[idx])\n",
    "        else:\n",
    "            return dict(X=img)\n",
    "\n",
    "    def sample(self, n, seed=42):\n",
    "        \"\"\"\n",
    "        Sample random observations from the training set\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        # Pick indices\n",
    "        idx = np.arange(len(self.file_names))\n",
    "        idx_sample = np.random.choice(idx, n, replace=False)\n",
    "\n",
    "        # Select from the the filenames at random\n",
    "        subset_mol_one_hot = []\n",
    "        subset_mol_smiles = []\n",
    "        subset_assay_labels = []\n",
    "        subset_file_names = []\n",
    "        imgs = []\n",
    "        \n",
    "        for i in idx_sample:\n",
    "            X, file_name, subset_mol_one_hot, mol_smile, assay_label = self.__getitem__(i).values()\n",
    "            imgs.append(X.unsqueeze(0))\n",
    "            subset_mol_one_hot.append(mol_name)\n",
    "            subset_mol_smiles.append(mol_smile)\n",
    "            subset_assay_labels.append(assay_label)\n",
    "            subset_file_names.append(file_name)\n",
    "        \n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        return dict(X=imgs, \n",
    "                    file_name=subset_file_names,\n",
    "                    mols_one_hot=subset_mol_one_hot, \n",
    "                    mol_smile=subset_mol_smiles,\n",
    "                    assay_label=subset_assay_labels\n",
    "                ) \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
