
import os
from random import sample 
import torch

from models.vae.AE import *
from models.vae.sigmaVAE import *
from models.vae.VAE import *
from models.flow.glow import *

from data.dataset import *
from utils import *
from plot_utils import Plotter 
from torch.utils.data import Dataset
from torch.utils.tensorboard import SummaryWriter
import torch
import itertools 
import numpy as np
import json
from tqdm import tqdm
import time 

#TODO: implement early stopping 
#TODO: write description of the parameters
 

class Config:
    """
    Read a configuration file in .json format and wraps the hparams into a calss
    """
    def __init__(self, config_path):
        self.config_path = config_path
        with open(self.config_path) as f:
            self.params = json.load(f)
    
    def __getattr__(self, name):
        if name in self.params:
            return self.params[name]
        else:
            raise AttributeError("No such attribute: " + name)


class Trainer:
    """
    Class for training the model 
    """
    def __init__(self, config, train_mode = True, **kwargs):
        self.data_path = config.data_path
        self.num_epochs = config.num_epochs
        self.eval = config.eval
        self.eval_every = config.eval_every
        self.batch_size = config.batch_size
        self.shuffle = config.shuffle
        self.n_workers_loader = config.n_workers_loader
        self.model_name = config.model_name
        self.model_config = config.model_config
        self.generate = config.generate
        self.temperature = config.temperature  # Temperature to downscale the random samples from the prior
        self.augment_train = config.augment_train

        # Resume the training 
        self.checkpoint_path = config.checkpoint_path
        self.resume = config.resume 
        self.resume_checkpoint = config.resume_checkpoint
        self.resume_epoch = 1

        # Plot decisions
        self.experiment_name = config.experiment_name
        self.result_path = config.result_path
        self.img_plot = config.img_plot
        self.img_save = config.img_save

        # Set device
        self.device = self.set_device() 
        print(f'Working on device: {self.device}')

        self.model =  self.load_model().to(self.device)
        self.model = nn.DataParallel(self.model)
        
        # Prepare the data
        print('Lodading the data...') 
        self.training_set, self.validation_set, self.test_set, self.ood_set = self.create_torch_datasets()
        
        # Create data loaders 
        self.loader_train = torch.utils.data.DataLoader(self.training_set, batch_size=self.batch_size, shuffle=self.shuffle, 
                                                    num_workers=self.n_workers_loader, drop_last=False)
        self.loader_val = torch.utils.data.DataLoader(self.validation_set, batch_size=self.batch_size, shuffle=self.shuffle, 
                                                    num_workers=self.n_workers_loader, drop_last=False)
        self.loader_test = torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size, shuffle=self.shuffle, 
                                                    num_workers=self.n_workers_loader, drop_last=False)
        self.loader_ood = torch.utils.data.DataLoader(self.ood_set, batch_size=self.batch_size, shuffle=self.shuffle, 
                                                    num_workers=self.n_workers_loader, drop_last=False)
        print('Successfully loaded the data')

        # Setup the optimizer 
        self.lr = config.lr
        self.wd = config.wd
        self.step_size = config.step_size
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)
        self.warmup_steps = config.warmup_steps
        self.lr_scheduling = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=self.step_size)

        # If training is resumed from a checkpoint
        if self.resume:
            self.resume_epoch, self.dest_dir = self.model.module.load_checkpoints(self.resume_checkpoint, self.optimizer, self.lr_scheduling)

    def create_torch_datasets(self):
        """
        Create dataset and data loader compatible with the pytorch training loop 
        """
        # Create dataset objects for the three data folds
        cellpainting_ds = CellPaintingDataset(self.data_path, device=self.device, return_labels=False, augment_train=self.augment_train)
        training_set, validation_set, test_set, ood_set = cellpainting_ds.fold_datasets.values()
        return training_set, validation_set, test_set, ood_set

    
    def train(self):
        """
        Full training 
        """
        # Create result folder
        if not self.resume and self.img_save:
            print('Create output directories for the experiment')
            self.dest_dir = make_dirs(self.result_path, self.experiment_name, self.img_save)
        
        # Setup plotter and writer 
        self.plotter = Plotter(self.dest_dir)

        # Setup logger
        self.writer = SummaryWriter(os.path.join(self.dest_dir, 'logs'))
        
        print(f'Beginning training with epochs {self.num_epochs}')
        min_loss = np.inf
        for epoch in range(self.resume_epoch, self.num_epochs+1):
            print(f'Running epoch {epoch}')
            self.model.train()
            # Losses from the epoch
            losses, metrics = self.model.module.update_model(self.loader_train, epoch, self.optimizer) # Update run 
            for key in losses:
                self.writer.add_scalar(tag=f'train/{key}', scalar_value=losses[key], global_step=epoch)
            for key in metrics:
                self.writer.add_scalar(tag=f'train/{key}', scalar_value=metrics[key], global_step=epoch)

            # Evaluate
            if epoch % self.eval_every == 0:
                # Put the model in evaluate mode 
                self.model.eval()
                val_losses, metrics = self.model.module.evaluate(self.loader_val, self.validation_set, self.device)
                
                for key in val_losses:
                    self.writer.add_scalar(tag=f'val/{key}', scalar_value=val_losses[key], 
                                           global_step=epoch)
                for key in metrics:
                    self.writer.add_scalar(tag=f'val/{key}', scalar_value=metrics[key], global_step=epoch)

                val_loss = val_losses['loss']

                # Plot reconstruction
                original  = next(iter(self.loader_val))['X'][0].to(self.device).unsqueeze(0)  # Get the first element of the test batch 
                with torch.no_grad():
                    reconstructed = self.model.module.generate(original)
                self.plotter.plot_reconstruction(tensor_to_image(original), 
                                                 tensor_to_image(reconstructed), epoch, self.img_save, self.img_plot)
                del original
                del reconstructed
                    
                # Plot generation of sampled images 
                if self.generate:
                    sampled_img = tensor_to_image(self.model.module.sample(1, self.temperature))
                    self.plotter.plot_channel_panel(sampled_img, epoch, self.img_save, self.img_plot)
                    del sampled_img
                
                # Save the model if it is the best performing one 
                if val_loss < min_loss:
                    min_loss = val_loss
                    print(f'New best loss is {val_loss}')
                    # Save the model with lowest validation loss 
                    self.model.module.save_checkpoints(epoch, 
                                                        self.optimizer, 
                                                        self.lr_scheduling, 
                                                        metrics, 
                                                        losses, 
                                                        val_losses, 
                                                        self.checkpoint_path, 
                                                        self.dest_dir)

                    print(f"Save new checkpoint at {self.checkpoint_path}")

            # Scheduler step at the end of the epoch 
            if epoch <= self.warmup_steps:
                self.optimizer.param_groups[0]["lr"] = self.lr * min(1., self.warmup_steps/epoch)
            else:
                self.lr_scheduling.step()


    def set_device(self):
        """
        Set cpu or gpu device for data and computations 
        """
        device = "cuda" if torch.cuda.is_available() else "cpu"
        return device
    
    def load_model(self):
        """
        Load the model from the dedicated library
        """
        models = {'AE':AE, 
        'VAE':VAE,'SigmaVAE': SigmaVAE,
        'Glow': Glow}
        model = models[self.model_name]
        return model(**self.model_config)
    

        
def gaussian_nll(mu, log_sigma, x):
    """
    Implement Gaussian nll loss
    """
    return 0.5 * torch.pow((x - mu) / log_sigma.exp(), 2) + log_sigma + 0.5 * np.log(2 * np.pi)

def softclip(tensor, min):
    """ Clips the tensor values at the minimum value min in a softway. Taken from Handful of Trials """
    result_tensor = min + F.softplus(tensor - min)

    return result_tensor
