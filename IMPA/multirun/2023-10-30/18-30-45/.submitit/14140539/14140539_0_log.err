GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in ../project_folder/experiments/20231030_bbbc021_try/wandb/run-20231030_183345-0cu3fdfj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run possessed-seance-17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/allepalma/bbbc021_try
wandb: üöÄ View run at https://wandb.ai/allepalma/bbbc021_try/runs/0cu3fdfj
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type         | Params
--------------------------------------------------
0 | embedding_matrix | Embedding    | 960   
1 | generator        | DataParallel | 24.3 M
2 | style_encoder    | DataParallel | 14.3 M
3 | discriminator    | DataParallel | 14.3 M
4 | mapping_network  | DataParallel | 11.3 K
--------------------------------------------------
53.0 M    Trainable params
960       Non-trainable params
53.0 M    Total params
211.840   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.

  0%|          | 0/103 [00:00<?, ?it/s][A  0%|          | 0/103 [09:24<?, ?it/s]
wandb: Waiting for W&B process to finish... (success).
wandb: üöÄ View run possessed-seance-17 at: https://wandb.ai/allepalma/bbbc021_try/runs/0cu3fdfj
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ../project_folder/experiments/20231030_bbbc021_try/wandb/run-20231030_183345-0cu3fdfj/logs
