GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
You are using a CUDA device ('NVIDIA A100-PCIE-40GB MIG 3g.20gb') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in ../project_folder/experiments/20231107_bbbc021_try/wandb/run-20231107_222754-y3gluxzh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sound-52
wandb: â­ï¸ View project at https://wandb.ai/allepalma/bbbc021_try
wandb: ğŸš€ View run at https://wandb.ai/allepalma/bbbc021_try/runs/y3gluxzh
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:630: Checkpoint directory ../project_folder/experiments/20231107_bbbc021_try/hydra_checkpoints exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-dcb1e9fb-f405-5de7-ab48-2aed70ecbcc6]

  | Name             | Type         | Params
--------------------------------------------------
0 | embedding_matrix | Embedding    | 960   
1 | generator        | DataParallel | 24.3 M
2 | style_encoder    | DataParallel | 14.3 M
3 | discriminator    | DataParallel | 14.3 M
4 | mapping_network  | DataParallel | 11.3 K
--------------------------------------------------
53.0 M    Trainable params
960       Non-trainable params
53.0 M    Total params
211.840   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:367: `ModelCheckpoint(monitor='fid_transformations')` could not find the monitored key in the returned metrics: ['D/latent_real', 'D/latent_fake', 'D/latent_reg', 'G/latent_adv', 'G/latent_sty', 'G/latent_ds', 'G/latent_cyc', 'G/lambda_ds', 'epoch', 'step']. HINT: Did you call `log('fid_transformations', value)` in the `LightningModule`?
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 14164533 ON gpusrv51 CANCELLED AT 2023-11-07T23:08:50 ***
slurmstepd: error: *** STEP 14164533.0 ON gpusrv51 CANCELLED AT 2023-11-07T23:08:50 ***
[rank: 0] Received SIGTERM: 15
Bypassing SIGTERM: 15
submitit WARNING (2023-11-07 23:08:50,796) - Bypassing signal SIGTERM
submitit WARNING (2023-11-07 23:08:50,799) - Bypassing signal SIGCONT
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:       D/latent_fake â–‚â–â–‚â–â–â–‚â–‚â–â–‚â–â–â–‚â–â–â–ˆâ–â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–‚â–
wandb:       D/latent_real â–‚â–‚â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        D/latent_reg â–ƒâ–†â–…â–„â–ƒâ–„â–ƒâ–‚â–ˆâ–‚â–‚â–â–ƒâ–‚â–â–ƒâ–‚â–‚â–â–…â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–‚â–‚â–‚â–â–â–ƒâ–‚â–
wandb:         G/lambda_ds â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:        G/latent_adv â–â–â–â–â–â–‚â–â–â–â–â–â–ˆâ–â–â–…â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:        G/latent_cyc â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb:         G/latent_ds â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–…â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆ
wandb:        G/latent_sty â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:               epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:       D/latent_fake 0.69365
wandb:       D/latent_real 0.00031
wandb:        D/latent_reg 0.00116
wandb:         G/lambda_ds 0.9513
wandb:        G/latent_adv 0.70172
wandb:        G/latent_cyc 0.21836
wandb:         G/latent_ds 38.21312
wandb:        G/latent_sty 0.13503
wandb:               epoch 8
wandb: trainer/global_step 4869
wandb: 
wandb: ğŸš€ View run toasty-sound-52 at: https://wandb.ai/allepalma/bbbc021_try/runs/y3gluxzh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ../project_folder/experiments/20231107_bbbc021_try/wandb/run-20231107_222754-y3gluxzh/logs
