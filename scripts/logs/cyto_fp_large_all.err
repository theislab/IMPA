/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python ../IMPA/main_hydra.py config=REBUTTAL_bbbc021_large_ ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in ../project_folder/experiments/20240903_c43e2460-7892-4af6-a551-39c3f67bb651_bbbc021_unannotated_large_cytoB/wandb/run-20240903_104058-3rzmxucy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run c43e2460-7892-4af6-a551-39c3f67bb651
wandb: ⭐️ View project at https://wandb.ai/allepalma/bbbc021
wandb: 🚀 View run at https://wandb.ai/allepalma/bbbc021/runs/3rzmxucy
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type         | Params
--------------------------------------------------
0 | embedding_matrix | Embedding    | 90.1 K
1 | generator        | DataParallel | 24.3 M
2 | style_encoder    | DataParallel | 16.2 M
3 | discriminator    | DataParallel | 14.3 M
4 | mapping_network  | DataParallel | 66.6 K
--------------------------------------------------
54.9 M    Trainable params
90.1 K    Non-trainable params
55.0 M    Total params
219.926   Total estimated model params size (MB)
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:367: `ModelCheckpoint(monitor='fid_transformations')` could not find the monitored key in the returned metrics: ['D/latent_real', 'D/latent_fake', 'D/latent_reg', 'G/latent_adv', 'G/latent_sty', 'G/latent_ds', 'G/latent_cyc', 'G/lambda_ds', 'epoch', 'step']. HINT: Did you call `log('fid_transformations', value)` in the `LightningModule`?
Traceback (most recent call last):
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-pgrzm3ya'
`Trainer.fit` stopped: `max_epochs=200` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:       D/latent_fake ▄▄▂▁▆█▄▄▂▃▂▂▄▃▂▄▆▄▄▃▂▅▄▆▄▄▄▂▄▃▆▅▄▁▄▆▅▄▄▃
wandb:       D/latent_real ▄▅▄▂▂▁▅▂▃▃▂█▃▂▅▄▁▃▄▄▅▃▃▂▅▄▅▇▄▂▃▂▃▄▁▂▃▄▄▄
wandb:        D/latent_reg ▁▁█▆▃▄▅▄▆▇▆▅▅▅▅▅▄▆▅▅▆▆▄▅▅▄▅▅▅▅▄▄▅▅▅▃▅▄▃▄
wandb:         G/lambda_ds ███▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:        G/latent_adv ▂▂▂▄▃█▃▄▃▂▄▁▄▃▂▂▄▄▂▂▁▃▃▄▂▂▃▁▂▄▂▄▃▂▄▃▃▂▂▂
wandb:        G/latent_cyc ▆▅██▇▇▅▄▅▅▃▃▅▃▂▃▃▃▂▁▂▃▂▂▃▂▂▂▃▂▂▂▂▃▁▂▂▂▁▁
wandb:         G/latent_ds ▂▅▂▇▃▄▄▅██▅▅▄▅▁▃▅▃▄▃▃▃▆▄▄▂▄▄▅▃▃▅▄▄▁▁▂▃▂▃
wandb:        G/latent_sty █▃▃▂▂▄▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▃▁▁▁▁▁▁▁▁▁▁▂
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:       D/latent_fake 0.4841
wandb:       D/latent_real 0.44959
wandb:        D/latent_reg 0.04835
wandb:         G/lambda_ds -1e-05
wandb:        G/latent_adv 1.55969
wandb:        G/latent_cyc 0.11471
wandb:         G/latent_ds 0.22867
wandb:        G/latent_sty 0.04274
wandb:               epoch 199
wandb: trainer/global_step 118599
wandb: 
wandb: 🚀 View run c43e2460-7892-4af6-a551-39c3f67bb651 at: https://wandb.ai/allepalma/bbbc021/runs/3rzmxucy
wandb: ️⚡ View job at https://wandb.ai/allepalma/bbbc021/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyOTI5ODk3NA==/version_details/v5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ../project_folder/experiments/20240903_c43e2460-7892-4af6-a551-39c3f67bb651_bbbc021_unannotated_large_cytoB/wandb/run-20240903_104058-3rzmxucy/logs
