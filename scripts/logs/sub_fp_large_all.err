/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python ../IMPA/main_hydra.py config=REBUTTAL_bbbc021_large_ ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
You are using a CUDA device ('NVIDIA A100-PCIE-40GB MIG 3g.20gb') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in ../project_folder/experiments/20240902_9dede8fc-6615-4f23-8299-bdd7bb52d920_bbbc021_unannotated_large_subset/wandb/run-20240902_170405-292bcxu7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 9dede8fc-6615-4f23-8299-bdd7bb52d920
wandb: ⭐️ View project at https://wandb.ai/allepalma/bbbc021
wandb: 🚀 View run at https://wandb.ai/allepalma/bbbc021/runs/292bcxu7
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-f7bec6fe-16b8-510a-991f-24d2d00e9899]

  | Name             | Type         | Params
--------------------------------------------------
0 | embedding_matrix | Embedding    | 24.6 K
1 | generator        | DataParallel | 24.3 M
2 | style_encoder    | DataParallel | 16.2 M
3 | discriminator    | DataParallel | 14.3 M
4 | mapping_network  | DataParallel | 66.6 K
--------------------------------------------------
54.9 M    Trainable params
24.6 K    Non-trainable params
54.9 M    Total params
219.533   Total estimated model params size (MB)
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:367: `ModelCheckpoint(monitor='fid_transformations')` could not find the monitored key in the returned metrics: ['D/latent_real', 'D/latent_fake', 'D/latent_reg', 'G/latent_adv', 'G/latent_sty', 'G/latent_ds', 'G/latent_cyc', 'G/lambda_ds', 'epoch', 'step']. HINT: Did you call `log('fid_transformations', value)` in the `LightningModule`?
`Trainer.fit` stopped: `max_epochs=200` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: \ 0.014 MB of 4.100 MB uploaded (0.000 MB deduped)wandb: | 0.014 MB of 4.100 MB uploaded (0.000 MB deduped)wandb: / 4.100 MB of 4.100 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:       D/latent_fake ▅▃▃█▂▃▃▁▅▃▆▃▂▂▆▆▄▅▃▅▂▄▆▇▃▁▃▂▅▃▁▃▂▃▃▅▂▂▃▄
wandb:       D/latent_real ▄▇▃▁▄▂▄▄▄▅▃▆▅▅▂▄▄▄▄▃▆▆▃▂▄▆▅▆▃▆█▄▅▂▄▂▅▅▅▄
wandb:        D/latent_reg ▁▃▄▅▆▅▇▇▅█▅▆▆▆▄▅▅▆▄▅▆▆▅▃▅▇▆▇▄▆▆▅▇▆▇▅▇▇▆▅
wandb:         G/lambda_ds ███▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:        G/latent_adv ▂▁▃█▂▅▂▂▃▁▃▁▂▂▄▄▂▂▂▃▁▂▃▄▂▂▂▁▃▂▁▃▂▄▂▃▁▁▂▂
wandb:        G/latent_cyc ▇▅▄██▆▄▇▆▄▂▄▃▆▄▅▃▃▃▃▂▃▃▂▃▃▃▂▂▃▂▂▃▃▂▂▂▁▁▁
wandb:         G/latent_ds ▁▄▇▆█▆▇▆█▇▇▆▇▇▇▆▇▆▆▆▇▅▅███▇▆▆█▆▇█▅▇▆▇▅▆▇
wandb:        G/latent_sty █▃▃▂▂▂▃▅▃▃▂▄▃▃▃▃▂▂▂▅▂▂▂▁▁▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:       D/latent_fake 0.20262
wandb:       D/latent_real 0.5357
wandb:        D/latent_reg 0.08797
wandb:         G/lambda_ds -1e-05
wandb:        G/latent_adv 1.73995
wandb:        G/latent_cyc 0.11613
wandb:         G/latent_ds 0.23864
wandb:        G/latent_sty 0.04489
wandb:               epoch 199
wandb: trainer/global_step 118599
wandb: 
wandb: 🚀 View run 9dede8fc-6615-4f23-8299-bdd7bb52d920 at: https://wandb.ai/allepalma/bbbc021/runs/292bcxu7
wandb: ️⚡ View job at https://wandb.ai/allepalma/bbbc021/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyOTI5ODk3NA==/version_details/v5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ../project_folder/experiments/20240902_9dede8fc-6615-4f23-8299-bdd7bb52d920_bbbc021_unannotated_large_subset/wandb/run-20240902_170405-292bcxu7/logs
