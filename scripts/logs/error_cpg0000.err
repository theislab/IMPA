An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python IMPA/main_hydra.py config=cpg0000 ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in project_folder/experiments/20240925_a703a47d-fe2e-43df-b221-f8d75233f48a_cpg0000/wandb/run-20240925_121018-63jlet59
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-snowball-20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/allepalma/cpg0000
wandb: üöÄ View run at https://wandb.ai/allepalma/cpg0000/runs/63jlet59
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type         | Params
--------------------------------------------------
0 | embedding_matrix | ModuleList   | 530 K 
1 | generator        | DataParallel | 24.3 M
2 | style_encoder    | DataParallel | 14.3 M
3 | discriminator    | DataParallel | 14.7 M
4 | mapping_network  | DataParallel | 132 K 
--------------------------------------------------
53.5 M    Trainable params
530 K     Non-trainable params
54.0 M    Total params
215.976   Total estimated model params size (MB)
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

  0%|          | 0/885 [00:00<?, ?it/s][A  0%|          | 0/885 [00:05<?, ?it/s]
Error executing job with overrides: ['config=cpg0000']
Traceback (most recent call last):
  File "/ictstr01/home/icb/alessandro.palma/environment/IMPA/IMPA/IMPA/main_hydra.py", line 86, in main
    trainer.fit(model=solver,
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in fit
    call._call_and_handle_interrupt(
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 581, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 990, in _run
    results = self._run_stage()
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1036, in _run_stage
    self.fit_loop.run()
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 203, in run
    self.on_advance_end()
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 373, in on_advance_end
    call._call_lightning_module_hook(trainer, "on_train_epoch_end")
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/icb/alessandro.palma/environment/IMPA/IMPA/IMPA/solver.py", line 174, in on_train_epoch_end
    metrics_dict = evaluate(self.nets,
  File "/home/icb/alessandro.palma/environment/IMPA/IMPA/IMPA/eval/eval.py", line 67, in evaluate
    z_emb = embedding_matrix(y_trg)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/nn/modules/module.py", line 372, in _forward_unimplemented
    raise NotImplementedError(f"Module [{type(self).__name__}] is missing the required \"forward\" function")
NotImplementedError: Module [ModuleList] is missing the required "forward" function

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/ictstr01/home/icb/alessandro.palma/environment/IMPA/IMPA/IMPA/main_hydra.py", line 86, in main
    trainer.fit(model=solver,
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in fit
    call._call_and_handle_interrupt(
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 581, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 990, in _run
    results = self._run_stage()
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1036, in _run_stage
    self.fit_loop.run()
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 203, in run
    self.on_advance_end()
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 373, in on_advance_end
    call._call_lightning_module_hook(trainer, "on_train_epoch_end")
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/icb/alessandro.palma/environment/IMPA/IMPA/IMPA/solver.py", line 174, in on_train_epoch_end
    metrics_dict = evaluate(self.nets,
  File "/home/icb/alessandro.palma/environment/IMPA/IMPA/IMPA/eval/eval.py", line 67, in evaluate
    z_emb = embedding_matrix(y_trg)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/nn/modules/module.py", line 372, in _forward_unimplemented
    raise NotImplementedError(f"Module [{type(self).__name__}] is missing the required \"forward\" function")
NotImplementedError: Module [ModuleList] is missing the required "forward" function

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ictstr01/home/icb/alessandro.palma/environment/IMPA/IMPA/IMPA/main_hydra.py", line 92, in <module>
    main()
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/hydra/_internal/utils.py", line 303, in run_and_report
    sys.exit(1)
  File "/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/wandb/sdk/lib/exit_hooks.py", line 36, in exit
    self._orig_exit(orig_code)  # type: ignore
SystemExit: 1
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: \ 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: | 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: / 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: - 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: \ 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:       D/latent_fake ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÑ
wandb:       D/latent_real ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÉ
wandb:        D/latent_reg ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñá‚ñÇ‚ñÜ‚ñÑ
wandb:         G/lambda_ds ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:        G/latent_adv ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÖ
wandb:        G/latent_cyc ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         G/latent_ds ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ
wandb:        G/latent_sty ‚ñà‚ñà‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:       D/latent_fake 0.91972
wandb:       D/latent_real 0.30872
wandb:        D/latent_reg 0.07533
wandb:         G/lambda_ds 0.98885
wandb:        G/latent_adv 1.41609
wandb:        G/latent_cyc 0.11877
wandb:         G/latent_ds 0.09416
wandb:        G/latent_sty 0.10571
wandb:               epoch 0
wandb: trainer/global_step 2229
wandb: 
wandb: üöÄ View run still-snowball-20 at: https://wandb.ai/allepalma/cpg0000/runs/63jlet59
wandb: Ô∏è‚ö° View job at https://wandb.ai/allepalma/cpg0000/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE4MTg2ODA3OQ==/version_details/v3
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: project_folder/experiments/20240925_a703a47d-fe2e-43df-b221-f8d75233f48a_cpg0000/wandb/run-20240925_121018-63jlet59/logs
