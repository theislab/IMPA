/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python ../IMPA/main_hydra.py config=REBUTTAL_bbbc021_large_ ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in ../project_folder/experiments/20240903_15bc0028-458a-4b12-ae4a-4162e90f8576_bbbc021_unannotated_large_subset_cytoB/wandb/run-20240903_104058-djzsq1e1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 15bc0028-458a-4b12-ae4a-4162e90f8576
wandb: ⭐️ View project at https://wandb.ai/allepalma/bbbc021
wandb: 🚀 View run at https://wandb.ai/allepalma/bbbc021/runs/djzsq1e1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type         | Params
--------------------------------------------------
0 | embedding_matrix | Embedding    | 24.6 K
1 | generator        | DataParallel | 24.3 M
2 | style_encoder    | DataParallel | 16.2 M
3 | discriminator    | DataParallel | 14.3 M
4 | mapping_network  | DataParallel | 66.6 K
--------------------------------------------------
54.9 M    Trainable params
24.6 K    Non-trainable params
54.9 M    Total params
219.533   Total estimated model params size (MB)
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:367: `ModelCheckpoint(monitor='fid_transformations')` could not find the monitored key in the returned metrics: ['D/latent_real', 'D/latent_fake', 'D/latent_reg', 'G/latent_adv', 'G/latent_sty', 'G/latent_ds', 'G/latent_cyc', 'G/lambda_ds', 'epoch', 'step']. HINT: Did you call `log('fid_transformations', value)` in the `LightningModule`?
`Trainer.fit` stopped: `max_epochs=200` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 4.743 MB uploaded (0.000 MB deduped)wandb: | 0.014 MB of 4.744 MB uploaded (0.000 MB deduped)wandb: / 4.744 MB of 4.744 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:       D/latent_fake █▄▄▂▃▃▂▃▃▂▂▃▃▂▂▂▅▃▄▁▂▁▂▃▃▂▁▅▁▃▁▁▃▂▂▁▂▂▃▂
wandb:       D/latent_real ▂▆▄▃▃▃▃▆▂▄▅▄▃▃▅▅▁▂▃▆▃▅▆▃▃▅▆▁█▅▄▆▄▄▅▄▃▂▃▄
wandb:        D/latent_reg ▁▁▃▄▃▅▄▅▃▄▃▄▃▄▆▅▃▄▄▆▅▅▆▄▆▅▇▄█▅▅▆▇▅▅▅▆▆▅▅
wandb:         G/lambda_ds ███▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:        G/latent_adv █▃▄▅▅▆▄▄▇▅▄▅▄▅▃▃▅▇▆▃▆▃▃▅▅▃▂▇▁▄▄▄▅▄▅▃▆▇▅▄
wandb:        G/latent_cyc █▃▃▃▃▃▂▃▅▃▂▂▂▃▂▂▂▂▃▂▄▁▂▃▂▂▃▃▃▂▂▁▂▂▂▂▂▂▂▂
wandb:         G/latent_ds ▁█▆▅█▇▄▃▆▄▇▅▅▆▆▆▆▄▅▆▅▄▅▅▇▇▅▇▅▇▅▆▅▃▇▅▄▇▆▅
wandb:        G/latent_sty █▃▂▂▂▂▂▂▂▁▃▁▂▂▂▁▂▁▂▁▁▁▁▃▂▁▁▁▃▁▁▁▂▁▂▂▁▂▁▂
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:       D/latent_fake 0.40884
wandb:       D/latent_real 0.3997
wandb:        D/latent_reg 0.09124
wandb:         G/lambda_ds -1e-05
wandb:        G/latent_adv 2.23977
wandb:        G/latent_cyc 0.10705
wandb:         G/latent_ds 0.23187
wandb:        G/latent_sty 0.02977
wandb:               epoch 199
wandb: trainer/global_step 118599
wandb: 
wandb: 🚀 View run 15bc0028-458a-4b12-ae4a-4162e90f8576 at: https://wandb.ai/allepalma/bbbc021/runs/djzsq1e1
wandb: ️⚡ View job at https://wandb.ai/allepalma/bbbc021/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyOTI5ODk3NA==/version_details/v5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ../project_folder/experiments/20240903_15bc0028-458a-4b12-ae4a-4162e90f8576_bbbc021_unannotated_large_subset_cytoB/wandb/run-20240903_104058-djzsq1e1/logs
