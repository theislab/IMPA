/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python ../IMPA/main_hydra.py config=REBUTTAL_bbbc021_large_ ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in ../project_folder/experiments/20240902_bd0b6e4d-8f56-4692-869c-8709c1bb7462_bbbc021_unannotated_large/wandb/run-20240902_164710-kaifm0yt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bd0b6e4d-8f56-4692-869c-8709c1bb7462
wandb: â­ï¸ View project at https://wandb.ai/allepalma/bbbc021
wandb: ğŸš€ View run at https://wandb.ai/allepalma/bbbc021/runs/kaifm0yt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type         | Params
--------------------------------------------------
0 | embedding_matrix | Embedding    | 90.1 K
1 | generator        | DataParallel | 24.3 M
2 | style_encoder    | DataParallel | 16.2 M
3 | discriminator    | DataParallel | 14.3 M
4 | mapping_network  | DataParallel | 66.6 K
--------------------------------------------------
54.9 M    Trainable params
90.1 K    Non-trainable params
55.0 M    Total params
219.926   Total estimated model params size (MB)
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:367: `ModelCheckpoint(monitor='fid_transformations')` could not find the monitored key in the returned metrics: ['D/latent_real', 'D/latent_fake', 'D/latent_reg', 'G/latent_adv', 'G/latent_sty', 'G/latent_ds', 'G/latent_cyc', 'G/lambda_ds', 'epoch', 'step']. HINT: Did you call `log('fid_transformations', value)` in the `LightningModule`?
slurmstepd: error: *** JOB 24280088 ON gpusrv11 CANCELLED AT 2024-09-02T16:58:14 ***
`Trainer.fit` stopped: `max_epochs=200` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:       D/latent_fake â–…â–…â–…â–‚â–ƒâ–…â–ƒâ–‡â–â–‚â–„â–ˆâ–ƒâ–‡â–…â–„â–â–…â–„â–ƒâ–…â–„â–„â–„â–…â–„â–ƒâ–„â–ƒâ–‚â–„â–„â–„â–…â–…â–ƒâ–„â–„â–„â–…
wandb:       D/latent_real â–ƒâ–†â–„â–†â–ˆâ–â–„â–‚â–…â–ˆâ–…â–â–„â–ƒâ–„â–…â–†â–…â–…â–‡â–…â–„â–ƒâ–„â–â–…â–†â–ƒâ–…â–…â–„â–…â–‡â–ˆâ–…â–‡â–†â–…â–‡â–‚
wandb:        D/latent_reg â–â–â–‚â–„â–…â–„â–…â–„â–ˆâ–‡â–†â–ƒâ–†â–…â–„â–…â–…â–…â–„â–†â–„â–„â–…â–…â–„â–„â–„â–…â–…â–…â–†â–„â–…â–…â–„â–…â–„â–…â–„â–ƒ
wandb:         G/lambda_ds â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:        G/latent_adv â–ƒâ–‚â–ƒâ–ƒâ–‚â–†â–…â–†â–‚â–â–ƒâ–…â–„â–„â–„â–ƒâ–‚â–ƒâ–ƒâ–â–„â–ƒâ–ƒâ–„â–ˆâ–ƒâ–‚â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–‚â–â–‚â–ƒâ–‚â–„
wandb:        G/latent_cyc â–†â–ƒâ–…â–ˆâ–‡â–†â–…â–…â–…â–…â–„â–…â–ƒâ–ƒâ–â–ƒâ–ƒâ–‚â–„â–‚â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–‚
wandb:         G/latent_ds â–â–ˆâ–†â–†â–ˆâ–‡â–†â–…â–‡â–ˆâ–†â–ˆâ–†â–…â–†â–‡â–†â–…â–†â–†â–†â–†â–…â–ˆâ–‡â–†â–†â–†â–‡â–…â–†â–…â–†â–‡â–„â–…â–…â–…â–†â–…
wandb:        G/latent_sty â–ˆâ–‚â–ƒâ–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:       D/latent_fake 0.4337
wandb:       D/latent_real 0.75943
wandb:        D/latent_reg 0.06593
wandb:         G/lambda_ds -1e-05
wandb:        G/latent_adv 1.01073
wandb:        G/latent_cyc 0.10221
wandb:         G/latent_ds 0.17535
wandb:        G/latent_sty 0.04834
wandb:               epoch 199
wandb: trainer/global_step 118599
wandb: 
wandb: ğŸš€ View run fc588378-f1f0-4cfb-84f5-92ed48f31a44 at: https://wandb.ai/allepalma/bbbc021/runs/w3nr10ww
wandb: ï¸âš¡ View job at https://wandb.ai/allepalma/bbbc021/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyOTI5ODk3NA==/version_details/v5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ../project_folder/experiments/20240902_fc588378-f1f0-4cfb-84f5-92ed48f31a44_bbbc021_unannotated_large/wandb/run-20240902_150706-w3nr10ww/logs
