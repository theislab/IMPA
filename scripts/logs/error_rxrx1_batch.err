An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python IMPA/main_hydra.py config=rxrx1_batch ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
You are using a CUDA device ('NVIDIA A100-PCIE-40GB MIG 3g.20gb') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in project_folder/experiments/20240603_ddd29f3c-8b6d-42be-b73a-13aeb6457349_rxrx1_batch/wandb/run-20240603_163110-8zy1rbb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-water-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/allepalma/rxrx1_batch
wandb: üöÄ View run at https://wandb.ai/allepalma/rxrx1_batch/runs/8zy1rbb6
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-c1581d36-4fc7-5d77-974f-dac8cbe41b19]

  | Name             | Type         | Params
--------------------------------------------------
0 | embedding_matrix | Embedding    | 300   
1 | generator        | DataParallel | 24.3 M
2 | style_encoder    | DataParallel | 14.3 M
3 | discriminator    | DataParallel | 14.3 M
4 | mapping_network  | DataParallel | 7.5 K 
--------------------------------------------------
53.0 M    Trainable params
0         Non-trainable params
53.0 M    Total params
211.838   Total estimated model params size (MB)
/home/icb/alessandro.palma/miniconda3/envs/IMPA_try/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
slurmstepd: error: *** JOB 21097134 ON gpusrv46 CANCELLED AT 2024-06-03T16:33:24 ***
